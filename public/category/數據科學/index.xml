<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>數據科學 | DLit-Hub</title>
    <link>http://localhost:56953/category/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8/</link>
      <atom:link href="http://localhost:56953/category/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8/index.xml" rel="self" type="application/rss+xml" />
    <description>數據科學</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 22 Dec 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:56953/media/icon_hu15379254428347791189.png</url>
      <title>數據科學</title>
      <link>http://localhost:56953/category/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8/</link>
    </image>
    
    <item>
      <title>數據科學聊聊：支持向量機（Support vector machine）</title>
      <link>http://localhost:56953/post/svm/</link>
      <pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:56953/post/svm/</guid>
      <description>&lt;h2 id=&#34;quick-look&#34;&gt;Quick look&lt;/h2&gt;
&lt;p&gt;支持向量機（SVM）是一種強大的監督式學習模型，廣泛應用於分類和回歸分析中。SVM 以其獨特的數學基礎和幾何直觀性，在高維數據空間中具有出色的性能，尤其適合處理少量數據和非線性問題。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;支持向量機的基本原理&#34;&gt;支持向量機的基本原理&lt;/h2&gt;
&lt;p&gt;SVM 的核心目標是找到一條分隔超平面（hyperplane），以最大化兩類數據點之間的間隔（margin）。以下是其運作的核心步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;分隔超平面
超平面是一個 d-1 維的空間，分隔 d 維特徵空間中的數據點。對於二分類問題，SVM 旨在找到一個最優超平面，使得兩類數據點之間的邊界最大化。數學表示為：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最大間隔原理
SVM 以「最大化分類間隔」為優化目標。目標函數為：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;硬邊界（Hard Margin）：數據點完全線性可分的情況，超平面將所有點正確分類。
軟邊界（Soft Margin）：允許少量錯誤分類的情況，引入松弛變量（slack variable）
ξ
ξ，以平衡分類錯誤和間隔最大化。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;核函數（Kernel Function）
SVM 支持非線性數據的分類，通過核函數將數據從低維空間映射到高維空間，使其在高維空間中線性可分。 常見的核函數包括：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;支持向量機的優缺點&lt;/p&gt;
&lt;p&gt;優點
高效處理高維數據
SVM 在高維空間中表現良好，並能有效處理高維度的特徵。
核函數允許其在非線性數據中找到最佳的分類邊界。
數學理論穩健
以凸優化問題為基礎，保證了全局最優解的存在。
適合小樣本學習
SVM 能在小數據集上表現出色，因為它的訓練過程只依賴於支持向量，而非所有數據點。
良好的泛化能力
最大間隔原理有效降低了模型的過擬合風險。
缺點
對超參數敏感
核函數的選擇、參數
γ
γ 和
C
C 的設置對模型性能有顯著影響，調參較為耗時。
計算成本高
當數據量較大時，計算支持向量的過程需要大量的內存和時間。
隨著數據量增大，SVM 的訓練速度可能急劇下降。
難以解釋
在高維空間中，分類邊界的可視化和解釋較為困難。
對噪音敏感
SVM 在處理存在噪音的數據時，可能表現不佳，尤其是在軟邊界中，噪音數據可能影響超平面的選擇。
3. SVM 的應用場景&lt;/p&gt;
&lt;p&gt;(1) 分類任務
文本分類：垃圾郵件過濾、情感分析等。
圖像分類：目標檢測、人臉識別。
醫療診斷：癌症預測、基因表達分類。
(2) 回歸任務
SVM 可應用於回歸分析（SVR, Support Vector Regression），用於預測連續值。
如房價預測、股票市場分析。
(3) 異常檢測
在監測數據中識別異常點，如網絡入侵檢測、機器故障檢測。
4. SVM 的未來發展與改進方向&lt;/p&gt;
&lt;p&gt;處理大規模數據
隨著數據量的增加，開發分布式和並行化 SVM 訓練算法成為重點。
如使用梯度下降法來替代二次規劃，降低計算成本。
結合深度學習
將 SVM 作為深度神經網絡的輸出層，結合深度學習的特徵提取能力，提高分類性能。
更靈活的核函數
開發自適應核函數，使其能自動選擇適合當前數據分布的核方法。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>數據科學技術盤點:主成分分析？</title>
      <link>http://localhost:56953/post/data_pca/</link>
      <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:56953/post/data_pca/</guid>
      <description>&lt;h2 id=&#34;quick-look&#34;&gt;Quick look&lt;/h2&gt;
&lt;p&gt;主成分分析（Principal component analysis, PCA）是一種強大且直觀的數據降維技術，被廣泛應用於臨床研究與醫學數據分析中。作為醫師，在日常工作中，我們經常面對大量的病患數據，例如基因表達數據、影像數據、實驗室檢驗值等。PCA能夠幫助我們提取數據的關鍵特徵，將高維度的資料轉換成易於解釋的低維度數據，從而揭示潛在的生物學意義。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;想要瞭接基本概念，可以看&lt;strong&gt;StatQuest&lt;/strong&gt;的&lt;a href=&#34;https://www.youtube.com/watch?v=FgakZw6K1QQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;影片&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;想快速了解數學概念的，可以看李政軒老師的&lt;a href=&#34;https://www.youtube.com/watch?v=JUPU8mJryL4&amp;amp;t=234s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;影片&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;主成分分析的數學基礎&#34;&gt;主成分分析的數學基礎&lt;/h1&gt;
&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;PCA的目的是將高維數據投影到一組新的互相正交的坐標軸上（稱為主成分），這些軸的排列方式使得：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一個主成分最大程度地捕捉數據的變異性；&lt;/li&gt;
&lt;li&gt;第二個主成分捕捉剩餘變異性，且與第一主成分正交，依此類推。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;也就是在一堆雜亂的數據中，找到變異程度最大的主軸來最大程度的代表這群資料，接著PCA會逐步找到與這些主軸正交的其他軸，這些軸共同構成一組新的坐標系。而所有原始數據的值都會重新投影到新的軸上。每個空間中的數據點可以被所有主成分&lt;span style=&#34;color: red; font-weight: bold&#34;&gt;&lt;strong&gt;線性加成&lt;/strong&gt;&lt;/span&gt;的方式來還原原始數據。整個過程可以大致簡化成如下的公式：&lt;/p&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
S = C_{\text{mean}} + t_1C_1 + t_2C_2 + \dots \text{(t:主成分權重; C:主成分)}
$$
&lt;/div&gt;
&lt;h2 id=&#34;pca的目的&#34;&gt;PCA的目的&lt;/h2&gt;
&lt;p&gt;我自己在分析基因數據的時候也很常用到PCA，甚至有些R bioconductor package也將PCA當成是標準前處理過程，但其實一開始都會思考，目前有其他降維工具，為何PCA還是很多人用？關於PCA的目的，初略有以下三點：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用降維，去除高關聯性多餘的維度&lt;/li&gt;
&lt;li&gt;利用視覺化在2維的座標上，發現離群值或異常值&lt;/li&gt;
&lt;li&gt;跟隨最大離散程度的軸向，保留原始訊息&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由此可見，PCA可以幫助在目前動則維度數百的資料集中，還能用視覺化分析主要數據，並且評估哪些特徵維度不具有分群的效果，而可以考慮刪除或用其他方式過濾。&lt;/p&gt;
&lt;h2 id=&#34;數學過程&#34;&gt;數學過程&lt;/h2&gt;
&lt;p&gt;整個過程概念是先找到空間中數據點的平均值&lt;em&gt;x-mean&lt;/em&gt;，即數據的中心點，然後以&lt;em&gt;x-mean&lt;/em&gt;為原點，尋找通過此點的一個方向向量&lt;em&gt;e&lt;/em&gt;，對於所有數據點&lt;em&gt;xi&lt;/em&gt;，計算他們相對於&lt;em&gt;x-mean&lt;/em&gt;的差值，並將這些差值投影到方向&lt;em&gt;e&lt;/em&gt;上，在&lt;em&gt;e&lt;/em&gt;方向上的投影值的總變異性(即投影值平方的總和)如果達到最大值，則&lt;em&gt;e&lt;/em&gt;為第一主成分。
這邊的&lt;em&gt;e&lt;/em&gt;為長度為1的單位向量，也就是需要經過正規化(normalization)處理，此單位化向量確保了比較不同方向時，投影的變異性(方差)完全由數據本身決定，而非向量的長度。&lt;/p&gt;
&lt;p&gt;所有數據點投影後的長度可以表達為：&lt;/p&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
\
e^T (\ x_1 - \bar{x}), e^T (\ x_2 - \bar{x}), \dots, e^T (\ x_n - \bar{x})
\
$$
&lt;/div&gt;
&lt;p&gt;而在&lt;em&gt;e&lt;/em&gt;上的數據變異度為：&lt;/p&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
\
\sigma^2 = \frac{1}{n} \sum_{i=1}^n \left( e^T (\ x_i - \bar{x}) - 0 \right)^2
\
$$
&lt;/div&gt;
&lt;p&gt;接下來把上面的公式拆開：
平方公式可以看成兩個相同的元素相乘。&lt;/p&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} \left(e^T (x_i - \bar{x}) \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left(e^T (x_i - \bar{x}) \right) \left(e^T (x_i - \bar{x}) \right)^T
$$
&lt;/div&gt;
&lt;p&gt;然而拆開為兩個矩陣，若要達到可以相乘的目的，後面的矩陣我們需要將其轉置。這邊我們回憶一下:
$$
(AB)^T=B^TA^T
$$
所以上面的公式可以變換成:&lt;/p&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
\frac{1}{n} \sum_{i=1}^n \left(e^T (x_i - \bar{x})\right) \left((x_i - \bar{x})^T e\right)
$$
&lt;/div&gt;
&lt;div style=&#34;overflow-x: scroll;&#34;&gt;
$$
=e^T \left( \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T \right) e = e^T \Sigma e
$$
&lt;/div&gt;
到這邊我們已經可以發現，中間大框框包圍著的部分其實就是&lt;span style=&#34;color: red; font-weight: bold&#34;&gt;共變異矩陣(covariance matrix)！&lt;/span&gt; 到此，我們已經可以看出來PCA要解的其實就是共變異矩陣。
&lt;p&gt;接下來我們要找到離散程度最大值出現的方向&lt;em&gt;e1&lt;/em&gt;:
$$
e_1 = \arg\max_{e} e^T \Sigma e, \quad e^T e = 1
$$&lt;/p&gt;
&lt;p&gt;這個算式若要求解，需要利用Lagrange function來針對約束條件來優化:
$$
f(e, \lambda) = e^T \Sigma e + \lambda (1 - e^T e)
$$
分別對參數進行偏微分後，得到：
$$
\frac{\partial f}{\partial e} = 2\Sigma e - 2\lambda e = 0 \implies \Sigma e = \lambda e
$$
及
$$
\frac{\partial f}{\partial \lambda} = 1 - e^T e = 0 \implies e^T e = 1
$$&lt;/p&gt;
&lt;p&gt;到此，線代魔王&lt;span style=&#34;color: red; font-weight: bold&#34;&gt;eigenvalue&lt;/span&gt;現身了，搞了那麼久，PCA原來是要我們解eigenvalue problem!!
而不要忘記了，針對對稱矩陣，eigenvalue都有實數解，所以這邊必定能找到eigenvector，也就是eigenvalue對應的方向向量。
接下來我們只要做簡單的移項，就不難發現，我們要求解最大離散程度的&lt;em&gt;e1&lt;/em&gt;，就是eigenvalue最大值的地方!
$$
\lambda = e^T \Sigma e
$$
而第二主軸也可以依據Lagrange function來求解，但解完偏微分之後會發現，不管是接下來的哪個次主軸，解的都是跟原來的公式一模一樣，所以我們只要求第二大的eigenvalue以及其對應的相向即可!&lt;/p&gt;
&lt;h2 id=&#34;實際運用&#34;&gt;實際運用&lt;/h2&gt;
&lt;p&gt;古人云：千言萬語不如一行code，我們來直接用R來玩玩看PCA是怎麼達成並且視覺化的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#加載數據&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;set.seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;123&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;rnorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;100&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ncol&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;colnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Feature1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Feature2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Feature3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;c1&#34;&gt;#標準化數據，將數據移到中心&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scaled_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;scale&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;center&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scale&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#計算共變異矩陣&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;cov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scaled_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;c1&#34;&gt;#計算共變異矩陣的特徵值和特徵向量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;eigen_result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;eigen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;eigenvalues&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eigen_result&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;出來的eigen vectors:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;[,1]&lt;/th&gt;
          &lt;th&gt;[,2]&lt;/th&gt;
          &lt;th&gt;[,3]&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;[1,]&lt;/td&gt;
          &lt;td&gt;0.6733474&lt;/td&gt;
          &lt;td&gt;-0.1690336&lt;/td&gt;
          &lt;td&gt;0.7197436&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;[2,]&lt;/td&gt;
          &lt;td&gt;-0.3533758&lt;/td&gt;
          &lt;td&gt;-0.9286934&lt;/td&gt;
          &lt;td&gt;0.1124905&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;[3,]&lt;/td&gt;
          &lt;td&gt;-0.6494065&lt;/td&gt;
          &lt;td&gt;0.3300852&lt;/td&gt;
          &lt;td&gt;0.6850657&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;而eigenvalues:
[1] 1.1505778   [2] 0.9801158   [3] 0.8693063&lt;/p&gt;
&lt;p&gt;此時我們試著使用R中的&lt;strong&gt;prcomp&lt;/strong&gt;來直接操作PCA:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#使用prcomp進行 PCA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pca_result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;prcomp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;center&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scale.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接著來同樣取出eigenvalues，看看是不是跟上面的一樣&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#取出eigenvalues prcomp用奇異值來解PCA，奇異值的平方就等於特徵值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;prcomp_eigenvalues&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pca_result&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sdev^2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;prcomp_eigenvalues&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;出來的eigenvalues:
[1] 1.1505778   [2] 0.9801158   [3] 0.8693063
跟上面操作的方式結果一至！&lt;/p&gt;
&lt;h2 id=&#34;pca的視覺化&#34;&gt;PCA的視覺化&lt;/h2&gt;
&lt;p&gt;接者我可以簡單的用&lt;strong&gt;ggfortify&lt;/strong&gt;套件來查看PCA的結果。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ggfortify&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;autoplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pca_result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.data.frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;colour&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#39;Feature1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;ggtitle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;PCA Projection&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;theme_minimal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  id=&#34;figure-pca-visualization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pca&#34; srcset=&#34;
               /post/data_pca/fig1_hu15827159032189168353.webp 400w,
               /post/data_pca/fig1_hu4818452470569895218.webp 760w,
               /post/data_pca/fig1_hu7070961658534383816.webp 1200w&#34;
               src=&#34;http://localhost:56953/post/data_pca/fig1_hu15827159032189168353.webp&#34;
               width=&#34;760&#34;
               height=&#34;656&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      PCA visualization
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;因為這個數據是隨機的，如果使用癌症基因數據，搭配一些顯著的臨床因子，視覺化後的分群應該會更漂亮。&lt;/p&gt;
&lt;h1 id=&#34;結論&#34;&gt;結論&lt;/h1&gt;
&lt;p&gt;主成分分析是一項將複雜數據轉化為清晰結構的強大工具，就像一個數據的解碼器，幫助我們從看似混亂的數據中提煉出最重要的線索。透過數學的魔力，我們不僅能找到數據中最大變異的方向，還能有效地降維並視覺化，讓原本高維的「迷霧森林」變成清晰的「航路圖」。&lt;/p&gt;
&lt;p&gt;在實際應用中，PCA不僅僅是一個數學公式，更是一種數據理解的思維方式。無論是在基因表達分析、醫學影像數據，還是多維臨床檢驗指標的綜合解讀中，PCA都能扮演一個「數據導航員」的角色，引領我們找到關鍵特徵，解決臨床問題，甚至發現潛在的生物學機制。&lt;/p&gt;
&lt;p&gt;當然PCA也有其缺點:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解釋性不足: PCA的主成分是數學上計算出的線性組合，雖然能有效地捕捉數據的變異性，但每個主成分的實際意義通常不直觀，難以直接對應到具體的物理或生物學意義。例如，在醫學數據中，第一主成分可能代表數個臨床指標的綜合作用，但無法具體指出是哪一個指標在起主導作用。&lt;/li&gt;
&lt;li&gt;假設數據線性: PCA基於線性代數方法，假設數據之間的關係是線性的。然而，在很多實際問題中（如基因調控網絡、非線性動力學模型等），數據之間的關係可能是高度非線性的，這會導致PCA無法有效捕捉數據的真實結構。&lt;/li&gt;
&lt;li&gt;對數據尺度敏感: 如果數據的尺度(scale)差異很大，PCA的結果會嚴重受到影響。例如，一個變數的數值範圍是0-1，而另一個變數範圍是1-1000，後者會在主成分計算中占據主導地位。因此，PCA通常需要在降維前對數據進行標準化（Standardization），否則結果可能不準確。&lt;/li&gt;
&lt;li&gt;對離群值敏感: PCA對於數據中的離群值(outliers)非常敏感，這是因為離群值會極大地影響共變異矩陣的計算，進而改變主成分的方向。如果數據中存在明顯的離群值，PCA的結果可能會失真，導致降維效果變差。&lt;/li&gt;
&lt;li&gt;無法處理稀疏性問題: 在處理高維度但稀疏的數據（如基因表達數據或文本數據）時，PCA可能不是最佳選擇。這是因為PCA的降維過程會壓縮所有特徵的信息，導致稀疏結構丟失，無法保留某些特徵的稀疏性。&lt;/li&gt;
&lt;li&gt;依賴均值與共變異: PCA基於數據的均值和共變異矩陣，假設數據的主要信息可以用這些統計量來描述。如果數據的分佈偏離高斯分佈或共變異矩陣無法很好地捕捉數據結構，PCA的效果可能會受到限制。&lt;/li&gt;
&lt;li&gt;僅考慮變異性，無法考慮分類信息: PCA僅最大化數據的變異性，並不考慮下游任務(如分類或聚類)的需求。因此，如果主要目的是分類或分群，PCA降維後的特徵可能無法有效提高模型的性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在實際使用時，需要結合數據特性、應用場景以及模型需求，選擇適合的降維方法，必要時可考慮其他技術（如t-SNE、UMAP或LDA）作為替代或輔助方法。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
