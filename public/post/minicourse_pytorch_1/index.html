<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: September 12, 2025 --><html lang="en-us" >


<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Yang-Hong Dai 戴揚紘" />





  

<meta name="description" content="" />



<link rel="alternate" hreflang="en-us" href="http://localhost:1313/post/minicourse_pytorch_1/" />
<link rel="canonical" href="http://localhost:1313/post/minicourse_pytorch_1/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu7094183078009362424.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu6717257383916802474.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />

  <meta property="twitter:site" content="@GetResearchDev" />
  <meta property="twitter:creator" content="@GetResearchDev" />
<meta property="twitter:image" content="http://localhost:1313/post/minicourse_pytorch_1/featured.jpg" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="BioMed Communications" />
<meta property="og:url" content="http://localhost:1313/post/minicourse_pytorch_1/" />
<meta property="og:title" content="迷你課程:PyTorch-1 | BioMed Communications" />
<meta property="og:description" content="" /><meta property="og:image" content="http://localhost:1313/post/minicourse_pytorch_1/featured.jpg" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-09-12T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2025-09-12T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/post/minicourse_pytorch_1/"
  },
  "headline": "迷你課程:PyTorch-1",
  
  "image": [
    "http://localhost:1313/post/minicourse_pytorch_1/featured.jpg"
  ],
  
  "datePublished": "2025-09-12T00:00:00Z",
  "dateModified": "2025-09-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "戴揚紘"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "BioMed Communications",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/media/icon_hu5086867786821458968.png"
    }
  },
  "description": ""
}
</script>

  

  




  
  
  

  
  

  


  
  <title>迷你課程:PyTorch-1 | BioMed Communications</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="8ba652281856822cf2e9f52f84bd4c75" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">BioMed Communications</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">BioMed Communications</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/tour"><span>導覽</span></a>
          </li>

          
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/post"><span>文章</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/people"><span>人物</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/event"><span>消息</span></a>
          </li>

          
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/tech"><span>新技術分享</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        
          
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  



<div class="article-container pt-3">
  <h1>迷你課程:PyTorch-1</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/yang-hong-dai-%E6%88%B4%E6%8F%9A%E7%B4%98/">Yang-Hong Dai 戴揚紘</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Sep 12, 2025
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  
    <span class="middot-divider"></span>
    <a href="/post/minicourse_pytorch_1/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/pytorch%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B/">PyTorch迷你課程</a></span>
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 192px;">
  <div style="position: relative">
    <img src="/post/minicourse_pytorch_1/featured_hu14378400816662828034.webp" width="720" height="192" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="quick-look">Quick look</h2>
<p>參考了這個教學<a href="https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors" target="_blank" rel="noopener">網站</a>，以及我自己在Udemy購買的線上課程</p>
<h2 id="什麼是-pytorch能做什麼為何用它">什麼是 PyTorch？能做什麼？為何用它？</h2>
<p>PyTorch 是一個開源的機器學習／深度學習框架，讓你用 Python 操作資料、撰寫與訓練模型。
被大量公司與研究機構採用（Meta、Tesla、Microsoft、OpenAI…），從自駕電腦視覺到農業電腦視覺皆可見。
優點：</p>
<ol>
<li>易上手、研究友善（動態計算圖、Pythonic）。</li>
<li>GPU 加速與自動微分幾乎都幫你處理好。</li>
<li>研究社群活躍、資源多（Papers with Code 使用率高）。</li>
</ol>
<p>資源:</p>
<ol>
<li>官方文件、論壇（discuss.pytorch.org）。</li>
<li>課程 GitHub／Discussions（如果跟課出現版本差異或錯誤）。</li>
</ol>
<hr>
<h2 id="安裝與版本">安裝與版本</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</span></span></code></pre></div><p>PyTorch 版本常影響 API 細節；只要 1.10+ 大多內容都相容。若你用 Colab，通常已內建。</p>
<hr>
<h2 id="張量tensor是什麼">張量（Tensor）是什麼？</h2>
<ul>
<li><code>張量</code> = 數字的容器。深度學習就是不斷對張量做運算。</li>
<li><code>標量 (scalar)</code>：<code>0</code> 維（例：7）</li>
<li><code>向量 (vector)</code>：<code>1</code> 維（例：[7,7]）</li>
<li><code>矩陣 (matrix)</code>：<code>2</code> 維（例：[[7,8],[9,10]]）</li>
<li><code>高維張量 (tensor)</code>：<code>任意維</code>（例：[1,3,224,224] 影像）</li>
</ul>
<h4 id="建立基本張量">建立基本張量</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 標量</span>
</span></span><span class="line"><span class="cl"><span class="n">scalar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">scalar</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>        <span class="c1"># tensor(7), 0</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">scalar</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>              <span class="c1"># 取出 Python 整數 7（只適用 1 個元素）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 向量</span>
</span></span><span class="line"><span class="cl"><span class="n">vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span> <span class="c1">#方向加上大小</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 1, torch.Size([2])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 矩陣</span>
</span></span><span class="line"><span class="cl"><span class="n">MATRIX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                       <span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">MATRIX</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">MATRIX</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 2, torch.Size([2,2])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 三維張量</span>
</span></span><span class="line"><span class="cl"><span class="n">TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">TENSOR</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">TENSOR</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 3, torch.Size([1,3,3])</span>
</span></span></code></pre></div><p>小技巧：看外層方括號數量可快速判斷維度數。所以三維張量那邊有三個<code>[[[</code>，所以為三維。</p>
<blockquote>
<p>在 PyTorch：</p>
</blockquote>
<blockquote>
<p>每次 tensor[i] = 沿著 第 0 維選取 index = i 的切片。
如果張量是多維的，選出來的結果會少一個維度</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">MATRIX</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>   <span class="c1"># (2, 2)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">MATRIX</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (2,)   &lt;- 取一列，少一維</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">TENSOR</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>    <span class="c1"># (1, 3, 3)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">TENSOR</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (3, 3) &lt;- 取一層，少一維</span>
</span></span></code></pre></div><hr>
<h2 id="建立常見張量隨機全-0全-1range">建立常見張量：隨機、全 0、全 1、range</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 隨機</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>                 <span class="c1"># 3x4，dtype= float32</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 指定影像形狀</span>
</span></span><span class="line"><span class="cl"><span class="n">random_image_size_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 與上面同義</span>
</span></span><span class="line"><span class="cl"><span class="n">random_image_size_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 全 0 / 全 1</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1">## dtype = float32</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 連續整數（左閉右開）</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 0..9</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 形狀對齊衍生</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>               <span class="c1"># 與 x 同形狀的全 0</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># 與 x 同形狀的全 1</span>
</span></span></code></pre></div><hr>
<h2 id="張量-3-大屬性shape--dtype--device">張量 3 大屬性：shape / dtype / device</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="c1">##requires_grad 表示是否需要追蹤梯度</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>   <span class="c1"># torch.Size([3,4])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>   <span class="c1"># torch.float32 (預設)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># cpu 或 cuda</span>
</span></span></code></pre></div><p>遇到錯誤先自問：形狀對嗎？資料型別對嗎？放在哪個裝置？（shape / dtype / device）</p>
<hr>
<h2 id="資料型別dtype">資料型別（dtype）</h2>
<p>常見：float32（預設）、float16、float64；整數 int8/16/32/64。
精度越低 ⇒ 計算越快、模型越小，但可能較不準。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">float_32</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">float_16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">float_16</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>             <span class="c1"># torch.float16</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 轉型</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#使用tensor.type()</span>
</span></span><span class="line"><span class="cl"><span class="n">x_half</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="c1">#也可以使用torch.half 與torch.float16同樣意思</span>
</span></span><span class="line"><span class="cl"><span class="n">x_int8</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
</span></span></code></pre></div><p>有時候不同的data type運算不一定會出現錯誤，例如<code>相加與相乘</code>，但仍需要小心data type的一致性。</p>
<hr>
<h2 id="張量運算加減乘除矩陣乘法">張量運算（加減乘除、矩陣乘法）</h2>
<h4 id="基本運算">基本運算</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">+</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">*</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 不會改變原 tensor，除非你重新賦值</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">-</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 函數式寫法</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</span></span></code></pre></div><p>上面的element-wise 乘法建議使用<code>*</code>，比較不容易出錯。</p>
<h4 id="元素乘-vs-矩陣乘">元素乘 vs 矩陣乘</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">*</span> <span class="n">a</span>                 <span class="c1"># 元素乘 -&gt; tensor([1,4,9])</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>    <span class="c1"># 矩陣乘 -&gt; 1*1 + 2*2 + 3*3 = 14</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="c1">#跟matmul同義</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">@</span> <span class="n">a</span>                 <span class="c1"># 等同 matmul</span>
</span></span></code></pre></div><p>使用matmul的矩陣相乘方式會比使用for loop 來運算要快很多。</p>
<h4 id="形狀錯誤是常態內部維度需相等">形狀錯誤是常態（內部維度需相等）</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 3x2</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span><span class="mi">10</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">11</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mf">12.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># 3x2</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.matmul(A, B)  # 會錯：3x2 @ 3x2 內維不合</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 轉置讓內維對上</span>
</span></span><span class="line"><span class="cl"><span class="n">B_T</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># 2x3</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B_T</span><span class="p">)</span>  <span class="c1"># 3x2 @ 2x3 -&gt; 3x3</span>
</span></span></code></pre></div><p>torch.mm 是 2D 專用矩陣乘（只接受 2D），torch.matmul 支援更一般情況。</p>
<p>所以，矩陣相乘時：</p>
<ol>
<li>內部維度要一致。</li>
<li>相乘後的矩陣維度為外層維度。</li>
</ol>
<hr>
<h2 id="線性層其實就是矩陣乘">線性層其實就是矩陣乘</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">A</span>                         <span class="c1"># A 形狀 [3,2]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    
</span></span></code></pre></div><p>若把 in_features 改成 3，輸入也要對應改成 [&hellip;, 3]（或經轉置／重塑）。</p>
<hr>
<h2 id="聚合minmaxmeansum與位置argminargmax">聚合（min/max/mean/sum）與位置（argmin/argmax）</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">();</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># torch.mean()內部的datatype 不能用在torch.long dtype</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 位置：找出對大值與最小值的位置</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">argmax</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span>   <span class="c1"># 8, 0</span>
</span></span></code></pre></div><hr>
<h2 id="形狀變換reshape--view--stack--squeeze--unsqueeze--permute">形狀變換：reshape / view / stack / squeeze / unsqueeze / permute</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">)</span>           <span class="c1"># [7]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#&gt; tensor([1., 2., 3., 4., 5., 6., 7.]) </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># reshape：回傳新的張量（可不連續記憶體時也能用）</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 這樣其實為多一層維度 tensor([[1., 2., 3., 4., 5., 6., 7.]]) 有沒有注意到外面多一層[]</span>
</span></span><span class="line"><span class="cl"><span class="n">x_r</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>               <span class="c1"># [1,7]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># view：回傳「同一塊資料的不同檢視」，**共享底層記憶體**</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>                         <span class="c1"># 會一併改到 x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 將多個張量疊成新維度</span>
</span></span><span class="line"><span class="cl"><span class="n">x_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [4,7]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># squeeze：移除所有長度為 1 的維度</span>
</span></span><span class="line"><span class="cl"><span class="n">x_sq</span> <span class="o">=</span> <span class="n">x_r</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>                         <span class="c1"># [7]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># unsqueeze：在指定位置加入長度為 1 的維度</span>
</span></span><span class="line"><span class="cl"><span class="n">x_us</span> <span class="o">=</span> <span class="n">x_sq</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                 <span class="c1"># [1,7]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># permute：重排軸順序（回傳 view，會共享資料）</span>
</span></span><span class="line"><span class="cl"><span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>                <span class="c1"># HWC height width channel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#Shift axis 0-&gt;1,1-&gt;2, 2-&gt;0</span>
</span></span><span class="line"><span class="cl"><span class="n">chw</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                   <span class="c1"># CHW</span>
</span></span></code></pre></div><p><code>view</code> 與 <code>permute</code> 產生的張量共享底層資料，改其中之一會同時改到另一個。</p>
<hr>
<h2 id="索引indexing">索引（Indexing）</h2>
<p><code>與 NumPy 類似</code>，外到內逐層 [] 或用逗號分隔維度：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># = x[0,0]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1"># 所有樣本的第 2 欄</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1"># 中間位置</span>
</span></span></code></pre></div><p>這邊如果用<code>:</code>全選，會發現多一個<code>[]</code> (eg <code>tensor([5])</code>)，如果用<code>0</code>來index就不會有 (<code>tensor(5)</code>)，需要注意。</p>
<hr>
<h2 id="與-numpy-互轉共享記憶體">與 NumPy 互轉（共享記憶體）</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># numpy -&gt; torch（**共享記憶體**）</span>
</span></span><span class="line"><span class="cl"><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>        <span class="c1"># 注意轉換後dtype 會跟 numpy 一致（float64-&gt;numpy default dtype）</span>
</span></span><span class="line"><span class="cl"><span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span> <span class="o">+</span> <span class="mi">1</span>                      <span class="c1"># 只改 arr，不會動 ten（因重新指派 arr）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># torch -&gt; numpy（**共享記憶體**）</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>                  <span class="c1"># float32 (pyTorch default)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>                      <span class="c1"># 也是 float32</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span>                          <span class="c1"># 重新指派，不會改 a</span>
</span></span></code></pre></div><p>若是裝置在 GPU 的張量，必須先 .cpu() 再 .numpy()。
如果 Tensor 在 <code>GPU（CUDA）</code> 上，必須先把資料拷回 CPU（<code>tensor.cpu()</code>），再 <code>.numpy()</code>。這一步會產生<code>新拷貝</code>，所以 不會共享記憶體；之後改 GPU 上的 tensor，NumPy 陣列不會跟著變，反之亦然。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 情況 A：CPU 張量 → 共享記憶體（零拷貝）</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>        <span class="c1"># 在 CPU</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>            <span class="c1"># a 與 t 共享同一塊 CPU 記憶體</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>                   <span class="c1"># 原地修改</span>
</span></span><span class="line"><span class="cl"><span class="c1"># a 會跟著變</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 情況 B：GPU 張量 → 必須先搬到 CPU（會拷貝，不共享）</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>  <span class="c1"># 在 GPU</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>               <span class="c1"># 先從 GPU 拷回 CPU，再轉 numpy（新拷貝）</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>                            <span class="c1"># 改 GPU 上的 t</span>
</span></span><span class="line"><span class="cl"><span class="c1"># a 不會變（因為 a 是拷貝，不共享）</span>
</span></span></code></pre></div><p>結論就是，想要 NumPy 與 Tensor 共享記憶體：Tensor 必須在 CPU，而且用 非複製的轉換（tensor.numpy()）。</p>
<hr>
<h2 id="可重現性random-seed">可重現性（Random Seed）</h2>
<p>先設定一個random seed，然後再更新random seed看訓練結果有沒有改善，並考慮持續更換。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 平常：兩個 rand 不同</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 設定種子：讓隨機序列可重現</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># 若再次設同 seed</span>
</span></span><span class="line"><span class="cl"><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>    <span class="c1"># -&gt; 與 C 完全相同</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">C</span> <span class="o">==</span> <span class="n">D</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>         <span class="c1"># True</span>
</span></span></code></pre></div><p>觀念：設一次 seed 會固定「隨機序列」，若你要兩次抽樣結果一模一樣，就在第二次抽前再設一次同樣的 seed。在notebook的狀態，如果只在一開始設定一次，這樣會無法得到兩個相同的random number。
真正嚴格可重現性還包含 cuDNN 設定、資料載入器隨機性等（見 PyTorch reproducibility guide）。</p>
<hr>
<h2 id="把張量與模型放到-gpu--mpsapple-silicon">把張量（與模型）放到 GPU / MPS（Apple Silicon）</h2>
<h4 id="檢查裝置與裝置無關寫法">檢查裝置與裝置無關寫法</h4>
<ol>
<li>核心概念</li>
</ol>
<ul>
<li>PyTorch 本身是一個深度學習框架，計算主要透過 張量運算 (tensor operations)。</li>
<li>在 CPU 上，這些運算會調用 BLAS、MKL 等數值庫。</li>
<li>在 GPU 上，PyTorch 會調用 NVIDIA 的 CUDA 生態系。</li>
</ul>
<ol start="2">
<li>底層技術</li>
</ol>
<ul>
<li>CUDA (Compute Unified Device Architecture)：NVIDIA 開發的 GPU 平行計算平台，讓程式能直接用 GPU 做矩陣運算。</li>
<li>cuDNN (CUDA Deep Neural Network library)：針對深度學習常用操作（卷積、池化、RNN 等）做高度優化的函式庫。</li>
<li>PyTorch GPU 版 就是透過 torch.cuda 把 tensor 運算轉換成 CUDA kernel，在 GPU 上執行。</li>
</ul>
<blockquote>
<p>什麼時候需要用到 Cloud Computing（AWS, 阿里雲, GCP 等）？</p>
</blockquote>
<blockquote>
<p>本地 GPU 不夠用，或你根本沒有 GPU 時，就需要雲端算力。常見情境：
模型太大 / 資料太多</p>
</blockquote>
<blockquote>
<p>本地電腦 GPU 記憶體不足（例如 8GB 無法跑大模型 BERT、ResNet-152、單細胞多組學數據）。</p>
</blockquote>
<blockquote>
<p>雲端可以選擇 16GB、40GB、80GB GPU（如 V100, A100, H100）。</p>
</blockquote>
<blockquote>
<p>多 GPU / 分布式訓練</p>
</blockquote>
<blockquote>
<p>本地通常只有 1 張 GPU，無法做大規模並行。
在雲端可以租一個多 GPU 節點（4–8 張卡），或整個集群，顯著加快訓練速度。</p>
</blockquote>
<blockquote>
<p>彈性使用 / 成本考量</p>
</blockquote>
<blockquote>
<p>如果只是偶爾需要 GPU，買一張昂貴的顯卡（比如 A100）不划算。</p>
</blockquote>
<blockquote>
<p>雲端隨用隨付：需要時租 1–2 天，不需要就釋放，節省硬體投資。</p>
</blockquote>
<blockquote>
<p>合作與可重現性</p>
</blockquote>
<blockquote>
<p>多人合作時，用雲端（AWS S3、阿里雲 OSS）存取資料與模型更方便。
也能部署 Notebook/環境給同事直接使用，避免本地環境差異。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span>              <span class="c1"># NVIDIA GPU</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s2">&#34;mps&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;mps&#34;</span>               <span class="c1"># Apple Silicon</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<h4 id="張量搬移">張量搬移</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>        <span class="c1"># 預設在 CPU</span>
</span></span><span class="line"><span class="cl"><span class="n">x_dev</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>             <span class="c1"># 複製到指定裝置（回傳新張量）</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x_dev</span><span class="p">,</span> <span class="n">x_dev</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 若要轉成 numpy，必須在 CPU</span>
</span></span><span class="line"><span class="cl"><span class="n">x_cpu_np</span> <span class="o">=</span> <span class="n">x_dev</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span></code></pre></div><p>規則：所有運算的張量要在同一裝置；要轉 NumPy 必須先 .cpu()。</p>
<h4 id="多-gpu">多 GPU</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>        <span class="c1"># 有幾張 GPU</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;cuda:0&#39;, &#39;cuda:1&#39; ... 0-based</span>
</span></span></code></pre></div><hr>
<h2 id="課程小結">課程小結</h2>
<ol>
<li>三問：出錯先問 shape? dtype? device?</li>
<li>view/permute 共享資料，修改其一會影響原張量；需要獨立請用 clone() 或 reshape()（非連續時 view 會失敗）。</li>
<li>矩陣乘法：內維必須相等；matmul 比 mm 通用（可處理 &gt;2D 與廣播）。</li>
<li>dtype 不合：float32 vs float16/64 常見；必要時轉型。</li>
<li>GPU→NumPy：先 .cpu() 再 .numpy()。</li>
<li>seed：要重現同一序列就重設同 seed；要不同結果就不要重設。</li>
</ol>
    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/python/">Python</a>
  
  <a class="badge badge-light" href="/tag/coding/">Coding</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F&amp;text=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F&amp;t=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F&amp;title=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1%20http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fminicourse_pytorch_1%2F&amp;title=%E8%BF%B7%E4%BD%A0%E8%AA%B2%E7%A8%8B%3APyTorch-1" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/yang-hong-dai-%E6%88%B4%E6%8F%9A%E7%B4%98/"><img class="avatar mr-3 avatar-circle" src="/author/yang-hong-dai-%E6%88%B4%E6%8F%9A%E7%B4%98/avatar_hu9497666732678154168.jpg" alt="Yang-Hong Dai 戴揚紘"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/yang-hong-dai-%E6%88%B4%E6%8F%9A%E7%B4%98/">Yang-Hong Dai 戴揚紘</a></h5>
      <h6 class="card-subtitle">主治醫師</h6>
      <p class="card-text">我的研究興趣為結合數據分析來探討各種癌症生物學。</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/yang-hong-dai-200b1aab/?originalSubdomain=uk" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://x.com/yanghongdai1" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=bH-1OKcAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/yanghong.dai" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  







  
  
  

  

  
  <section id="comments">
    
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "yhd" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  </section>
  










  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2025. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  









  <script id="dsq-count-scr" src="https://yhd.disqus.com/count.js" async></script>




  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.js"></script>



  <script src="/js/wowchemy-map.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.js" type="module"></script>


















</body>
</html>
